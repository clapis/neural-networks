{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar Tensor\n",
    "\n",
    "Defining a `Tensor` class which wraps a scalar value and supports the basic operations of addition and multiplication and respective gradients for backpropagation.  \n",
    "\n",
    "Also add support for the hyperbolic tangent which will be used as activation fuction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import deque\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, value, _inputs=()):\n",
    "        self.value = value\n",
    "        self.grad = 0.0\n",
    "        self._inputs = _inputs\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tensor( Value: {self.value}, Grad: {self.grad} )\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        result = Tensor(self.value + other.value, (self, other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad +=  result.grad\n",
    "            other.grad += result.grad\n",
    "\n",
    "        result._backward = _backward\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        result = Tensor(self.value * other.value, (self, other))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += result.grad * other.value\n",
    "            other.grad += result.grad * self.value\n",
    "        result._backward = _backward\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def tanh(self):\n",
    "        \n",
    "        result = Tensor(math.tanh(self.value), (self,))\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += result.grad * (1 - result.value ** 2)\n",
    "        result._backward = _backward\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def backward(self):\n",
    "        # topological sort\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._inputs:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # backward pass\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example `x1 * w1 + x2 * w2 + b`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y 0.7071067811865458\n",
      "x1: Tensor( Value: 2.0, Grad: -1.5000000000000073 )\n",
      "w1: Tensor( Value: -3.0, Grad: 1.0000000000000049 )\n",
      "x2: Tensor( Value: 0.0, Grad: 0.5000000000000024 )\n",
      "w2: Tensor( Value: 1.0, Grad: 0.0 )\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "x1 = Tensor(2.0)\n",
    "x2 = Tensor(0.0)\n",
    "\n",
    "# Weights\n",
    "w1 = Tensor(-3.0)\n",
    "w2 = Tensor(1.0)\n",
    "\n",
    "# Bias\n",
    "b = Tensor(6.88137358701954)\n",
    "\n",
    "# x1w1 + x2w2 + b\n",
    "y = (x1 * w1 + x2 * w2 + b).tanh()\n",
    "\n",
    "print(\"y\", y.value)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(\"x1:\", x1)\n",
    "print(\"w1:\", w1)\n",
    "print(\"x2:\", x2)\n",
    "print(\"w2:\", w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying results above with PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071066904050358\n",
      "x1: -1.5000003851533106\n",
      "w1: 1.0000002567688737\n",
      "x2: 0.5000001283844369\n",
      "w2: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x1 = torch.Tensor([2.0]).double()                       ; x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double()                       ; x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double()                      ; w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double()                       ; w2.requires_grad = True\n",
    "b = torch.Tensor([6.88137358701954]).double()           ; b.requires_grad = True\n",
    "\n",
    "y = torch.tanh(x1 * w1 + x2 * w2 + b)\n",
    "\n",
    "print(y.item())\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(\"x1:\", x1.grad.item())\n",
    "print(\"w1:\", w1.grad.item())\n",
    "print(\"x2:\", x2.grad.item())\n",
    "print(\"w2:\", w2.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron & Multilayer Perceptron (MLP)\n",
    "\n",
    "Now let's use the `Tensor` class to model an artificial `Neuron` and a multilayer percpetron (`MLP`)\n",
    "\n",
    "![neuron model](./neuron_model.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, nin):\n",
    "        self.weights = [Tensor(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.bias = Tensor(random.uniform(-1, 1))\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return sum([w * i for w, i in zip(self.weights, inputs)], self.bias).tanh()\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.weights + [self.bias]\n",
    "    \n",
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        results = [neuron(x) for neuron in self.neurons]\n",
    "        return results[0] if len(results) == 1 else results\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "    \n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts):\n",
    "        dimensions = [nin] + nouts\n",
    "        self.layers = [Layer(dimensions[i], dimensions[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying it on a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = MLP(3, [4, 4, 1])\n",
    "\n",
    "# Input values\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [-1.0, 1.0, -1.0]\n",
    "]\n",
    "\n",
    "# Target values\n",
    "ys = [1.0, -1.0, -1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Tensor( Value: 0.0025013354302307255, Grad: 1 )\n",
      "1 Tensor( Value: 0.00248839315820144, Grad: 1 )\n",
      "2 Tensor( Value: 0.0024755830264202986, Grad: 1 )\n",
      "3 Tensor( Value: 0.0024629030309516005, Grad: 1 )\n",
      "4 Tensor( Value: 0.002450351208047471, Grad: 1 )\n",
      "5 Tensor( Value: 0.002437925633147754, Grad: 1 )\n",
      "6 Tensor( Value: 0.0024256244199096226, Grad: 1 )\n",
      "7 Tensor( Value: 0.002413445719265785, Grad: 1 )\n",
      "8 Tensor( Value: 0.002401387718510419, Grad: 1 )\n",
      "9 Tensor( Value: 0.0023894486404117572, Grad: 1 )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Tensor( Value: 0.9660081266522845, Grad: -0.06798374669543095 ),\n",
       " Tensor( Value: -0.9946950235905366, Grad: 0.010609952818926738 ),\n",
       " Tensor( Value: -0.9723795825618414, Grad: 0.055240834876317146 ),\n",
       " Tensor( Value: 0.9789531248741878, Grad: -0.04209375025162432 )]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k in range(10):\n",
    "\n",
    "    # forward pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum([(prediction + target * -1) * (prediction + target * -1) for target, prediction in zip(ys, ypred)], Tensor(0))\n",
    "\n",
    "    # backward pass\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    for p in n.parameters():\n",
    "        p.value += -0.05 * p.grad \n",
    "\n",
    "    print(k, loss)\n",
    "\n",
    "ypred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
